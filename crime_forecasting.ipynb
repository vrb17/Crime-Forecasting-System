{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eeedfe-3601-4714-ae46-3340899e0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede6352-00c3-46be-9c83-a2f52063a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'crimedatadenver.csv'\n",
    "crime_data = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_remove = [\n",
    "    'OBJECTID', 'INCIDENT_ID', 'OFFENSE_ID', \n",
    "    'OFFENSE_CODE', 'OFFENSE_CODE_EXTENSION', 'DISTRICT_ID',\n",
    "    'LAST_OCCURRENCE_DATE', 'REPORTED_DATE', 'GEO_X', 'GEO_Y',\n",
    "    'NEIGHBORHOOD_ID', 'IS_CRIME', 'IS_TRAFFIC', 'VICTIM_COUNT', 'x', 'y'\n",
    "]\n",
    "\n",
    "crime_data_cleaned = crime_data.drop(columns=columns_to_remove)\n",
    "\n",
    "crime_data_cleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fef10-e556-4595-8116-e3b8099d8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data_cleaned.to_csv(\"cleaned_crime_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06459661-4ec9-4a8b-8f0a-b39f4822500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'cleaned_crime_data.csv'\n",
    "crime_data = pd.read_csv(file_path)\n",
    "\n",
    "db_path = 'crime_data.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('DROP TABLE IF EXISTS denver_crime_data')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE denver_crime_data (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    address TEXT,\n",
    "    crime_type TEXT,\n",
    "    precinct INTEGER,\n",
    "    first_occurrence_date TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "for _, row in crime_data.iterrows():\n",
    "    cursor.execute('''\n",
    "    INSERT INTO denver_crime_data (latitude, longitude, address, crime_type, precinct, first_occurrence_date)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        row['GEO_LAT'],\n",
    "        row['GEO_LON'],\n",
    "        row.get('address', ''),\n",
    "        row['OFFENSE_TYPE_ID'],\n",
    "        row['PRECINCT_ID'],\n",
    "        row.get('FIRST_OCCURRENCE_DATE', '')\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9dfe8-b7bd-4415-bda3-0b16061718c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "db_path = 'crime_data.db'\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "crime_df = pd.read_sql_query(\n",
    "    \"SELECT rowid, latitude, longitude, first_occurrence_date FROM denver_crime_data\", conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "print(crime_df.head())\n",
    "\n",
    "crime_df['first_occurrence_date'] = pd.to_datetime(crime_df['first_occurrence_date'], errors='coerce')\n",
    "\n",
    "crime_df = crime_df.dropna(subset=['first_occurrence_date'])\n",
    "\n",
    "two_years_ago = datetime.now() - timedelta(days=2 * 365)\n",
    "\n",
    "crime_df = crime_df[crime_df['first_occurrence_date'] >= two_years_ago]\n",
    "\n",
    "crime_df = crime_df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "crime_df = crime_df.drop_duplicates()\n",
    "\n",
    "crime_df = crime_df.head(10000)\n",
    "\n",
    "coordinates = crime_df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "coordinates_scaled = scaler.fit_transform(coordinates)\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "best_eps = 0.05\n",
    "best_min_samples = 5\n",
    "\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_min_samples, n_jobs=-1).fit(coordinates_scaled)\n",
    "\n",
    "crime_df['cluster'] = db.labels_\n",
    "\n",
    "print(f\"Number of clusters found: {len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)}\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(denver_crime_data);\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'cluster' not in columns:\n",
    "    conn.execute(\"ALTER TABLE denver_crime_data ADD COLUMN cluster INTEGER;\")\n",
    "    print(\"Added 'cluster' column to the table.\")\n",
    "else:\n",
    "    print(\"'cluster' column already exists in the table.\")\n",
    "\n",
    "for index, row in crime_df.iterrows():\n",
    "    conn.execute(\n",
    "        \"UPDATE denver_crime_data SET cluster = ? WHERE rowid = ?\",\n",
    "        (int(row['cluster']), int(row['id']))\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database updated with cluster labels.\")\n",
    "\n",
    "# Initialize the map centered around the mean coordinates\n",
    "map_center = [crime_df['latitude'].mean(), crime_df['longitude'].mean()]\n",
    "crime_map = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "# Use FastMarkerCluster for efficient marker visualization\n",
    "marker_data = [\n",
    "    (row['latitude'], row['longitude'])\n",
    "    for _, row in crime_df[crime_df['cluster'] != -1].iterrows()\n",
    "]\n",
    "FastMarkerCluster(marker_data).add_to(crime_map)\n",
    "\n",
    "crime_map.save(\"denver_crime_clusters.html\")\n",
    "print(\"Cluster map saved as denver_crime_clusters.html.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0b855-7559-4729-b185-6f07c1da1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import folium\n",
    "\n",
    "db_path = 'crime_data.db'\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "crime_df = pd.read_sql_query(\n",
    "    \"SELECT cluster, latitude, longitude, first_occurrence_date FROM denver_crime_data WHERE cluster IS NOT NULL AND cluster != -1\",\n",
    "    conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "crime_df['first_occurrence_date'] = pd.to_datetime(crime_df['first_occurrence_date'], errors='coerce')\n",
    "\n",
    "crime_df = crime_df.dropna(subset=['first_occurrence_date'])\n",
    "\n",
    "unique_clusters = crime_df['cluster'].unique()\n",
    "\n",
    "cluster_forecasts = {}\n",
    "cluster_centroids = {}\n",
    "\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_data = crime_df[crime_df['cluster'] == cluster_id]\n",
    "\n",
    "    time_series = (\n",
    "        cluster_data\n",
    "        .set_index('first_occurrence_date')\n",
    "        .resample('D')\n",
    "        .size()\n",
    "    )\n",
    "\n",
    "    # Handle cases where time series is too short for ARIMA\n",
    "    if len(time_series) < 5:\n",
    "        print(f\"Cluster {cluster_id} skipped due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    # Fit ARIMA model\n",
    "    try:\n",
    "        model = ARIMA(time_series, order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "       \n",
    "        # Forecast the next 30 days\n",
    "        forecast = model_fit.forecast(steps=30)\n",
    "        cluster_forecasts[cluster_id] = forecast\n",
    "       \n",
    "        # Calculate the centroid for the cluster\n",
    "        cluster_centroids[cluster_id] = [\n",
    "            cluster_data['latitude'].mean(),\n",
    "            cluster_data['longitude'].mean()\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for cluster {cluster_id}: {e}\")\n",
    "\n",
    "crime_map = folium.Map(location=[39.7392, -104.9903], zoom_start=12)\n",
    "\n",
    "# Add markers for each cluster with forecasted data\n",
    "for cluster_id, forecast in cluster_forecasts.items():\n",
    "    avg_forecast = forecast.mean()\n",
    "\n",
    "    # Determine marker size and color based on severity\n",
    "    size = 7 + (avg_forecast * 0.2)\n",
    "    size = min(max(size, 7), 10)\n",
    "    if avg_forecast > 10:\n",
    "        color = 'red'  # High risk\n",
    "    elif avg_forecast > 3:\n",
    "        color = 'orange'  # Medium risk\n",
    "    else:\n",
    "        color = 'green'  # Low risk\n",
    "\n",
    "    # Add marker to the map\n",
    "    folium.CircleMarker(\n",
    "        location=cluster_centroids[cluster_id],\n",
    "        radius=size,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(\n",
    "            f\"Hotspot {cluster_id}<br>Avg Forecast: {avg_forecast:.2f}<br>Forecast Values: {list(forecast)}\",\n",
    "            max_width=300\n",
    "        )\n",
    "    ).add_to(crime_map)\n",
    "\n",
    "crime_map.save(\"denver_crime_forecast_map.html\")\n",
    "print(\"Forecast map saved as denver_crime_forecast_map.html.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944cc719-e58c-4e77-bf8f-49c3bdf6522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "db_path = 'crime_data.db'\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "crime_data = pd.read_sql_query(\n",
    "    \"SELECT cluster, first_occurrence_date FROM denver_crime_data WHERE cluster IS NOT NULL AND cluster != -1\",\n",
    "    conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "crime_data['first_occurrence_date'] = pd.to_datetime(crime_data['first_occurrence_date'], errors='coerce')\n",
    "\n",
    "crime_data = crime_data.dropna(subset=['first_occurrence_date'])\n",
    "\n",
    "cluster_forecasts = {}\n",
    "evaluation_results = {}\n",
    "\n",
    "unique_clusters = crime_data['cluster'].unique()\n",
    "\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_data = crime_data[crime_data['cluster'] == cluster_id]\n",
    "\n",
    "    time_series = (\n",
    "        cluster_data\n",
    "        .set_index('first_occurrence_date')\n",
    "        .resample('D')\n",
    "        .size()\n",
    "    )\n",
    "\n",
    "    if len(time_series) < 10:\n",
    "        print(f\"Cluster {cluster_id} skipped due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    split_point = int(len(time_series) * 0.8)  # 80% for training, 20% for testing\n",
    "    train_data = time_series[:split_point]\n",
    "    test_data = time_series[split_point:]\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(train_data, order=(1, 1, 1))  # Adjust (p, d, q) if needed\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        forecast_steps = len(test_data)\n",
    "        forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "        mae = mean_absolute_error(test_data, forecast)\n",
    "        rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
    "\n",
    "        cluster_forecasts[cluster_id] = forecast\n",
    "        evaluation_results[cluster_id] = {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "        print(f\"Cluster {cluster_id}: MAE = {mae:.2f}, RMSE = {rmse:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for cluster {cluster_id}: {e}\")\n",
    "\n",
    "\n",
    "evaluation_df = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "evaluation_df.reset_index(inplace=True)\n",
    "evaluation_df.rename(columns={'index': 'cluster'}, inplace=True)\n",
    "\n",
    "evaluation_df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
    "print(\"Evaluation results saved to 'model_evaluation_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
